{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from data_process import DataHandle, get_task_data, get_word2idx\n",
    "from training_lib import get_model_inputs, trainer,generate_testing_result\n",
    "# !pip install tqdm\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "print('Libraries imported!')\n",
    "def myaccuracy(output, target):\n",
    "    print('get acc')\n",
    "    average = 'macro'\n",
    "    length = len(target)\n",
    "    if output.shape[-1] != 1:\n",
    "        output = torch.argmax(output, dim=1).float()\n",
    "\n",
    "    target = target.float()\n",
    "    output = output.view(target.shape)\n",
    "    predict = torch.round(output)\n",
    "    correct = (predict == target).float()\n",
    "    acc = correct.sum() / length\n",
    "\n",
    "    y_true = np.array(target)\n",
    "    y_pred = predict.detach().numpy()\n",
    "\n",
    "    matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred, average=average)\n",
    "    return acc, f1_score, matrix\n",
    "def word2vec_embedding(tokenized_corpus, embed_size=50, min_count=1, window=5):\n",
    "    sentences = tokenized_corpus\n",
    "    model = Word2Vec(sentences, min_count=min_count, window=window, size=embed_size)\n",
    "    # model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "    # train word vectors\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    # add the first vector as pading\n",
    "    embed_vectors = np.vstack([np.zeros((1, embed_size)), model.wv.vectors])\n",
    "    vocabulary = ['<pad>'] + model.wv.index2word\n",
    "    return embed_vectors, vocabulary\n",
    "# we fix the seeds to get consistent results\n",
    "SEED = 234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):\n",
    "        super(LSTM,self).__init__()\n",
    "        # embedding (lookup layer) layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,1,bidirectional=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim*2*2)\n",
    "        # output layer\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.out = nn.Linear(hidden_dim*2*2, num_classes)\n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "#         print(embedded)\n",
    "        states, hidden = self.lstm(embedded.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0],states[-1]], dim=1)\n",
    "        out = F.sigmoid(self.out(self.bn2(encoding)))\n",
    "\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class GRU(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):\n",
    "        super(GRU,self).__init__()\n",
    "        # embedding (lookup layer) layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # hidden layer\n",
    "#         self.gru = nn.GRU(embedding_dim,hidden_dim,1,bidirectional=True,dropout = 0.2)\n",
    "        self.gru = nn.GRU(embedding_dim,hidden_dim,1,bidirectional=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim*2*2)\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.out = nn.Linear(hidden_dim*2*2, num_classes)\n",
    "\n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = (self.embedding(x))\n",
    "        states, hidden = self.gru(embedded.permute([1, 0, 2]))#(sentenceL,BatchL,WordEemL)\n",
    "        encoding = F.leaky_relu(torch.cat([states[0],states[-1]], dim=1),0.1)\n",
    "        out = self.out(self.bn2(encoding))\n",
    "        out=F.sigmoid(out)     \n",
    "        return out\n",
    "    \n",
    "def accuracy(output, target):\n",
    "    predict = torch.round(torch.sigmoid(output))\n",
    "#     print(predict)\n",
    "    correct = (predict == target).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def embed_GRU_model(embedding, Vocabulary=100, EMBEDDING_DIM=50, HIDDEN_DIM=50, OUTPUT_DIM=1, lr=0.001,task='a'):\n",
    "    # embedding: np.array\n",
    "    # we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "    # Hidden_dim: dimensionality of the output of the second hidden layer\n",
    "    # OUTPUT_dim: the outut dimension is the number of classes, 1 for binary classification\n",
    "    assert embedding.shape[1] == EMBEDDING_DIM\n",
    "    model = GRU(vocab_size=Vocabulary,hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, num_classes=OUTPUT_DIM)\n",
    "    model.embedding.weight.data.copy_(torch.from_numpy(embedding))\n",
    "    model.embedding.weight.require_grad = False\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    # we use the Binary cross-entropy loss with sigmoid (applied to logits)\n",
    "    # Recall we did not apply any activation to our output layer, we need to make our outputs look like probality.\n",
    "    if task == 'c':\n",
    "        w = torch.Tensor([1.0,2.0,8.0])\n",
    "        loss_fn = nn.NLLLoss(weight=w)\n",
    "    if task =='b':\n",
    "#         w = torch.Tensor([9.0,1.0])\n",
    "        loss_fn = nn.BCELoss(weight=None)\n",
    "    if task =='a':\n",
    "        loss_fn = nn.BCELoss()\n",
    "    return model, optimizer, loss_fn\n",
    "\n",
    "def embed_LSTM_model(embedding, Vocabulary=100, EMBEDDING_DIM=50, HIDDEN_DIM=50, OUTPUT_DIM=1, lr=0.001,task='a'):\n",
    "    # embedding: np.array\n",
    "    # we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "    # Hidden_dim: dimensionality of the output of the second hidden layer\n",
    "    # OUTPUT_dim: the outut dimension is the number of classes, 1 for binary classification\n",
    "    assert embedding.shape[1] == EMBEDDING_DIM\n",
    "    model = LSTM(vocab_size=Vocabulary,hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, num_classes=OUTPUT_DIM)\n",
    "    model.embedding.weight.data.copy_(torch.from_numpy(embedding))\n",
    "    model.embedding.weight.require_grad = False\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    # we use the Binary cross-entropy loss with sigmoid (applied to logits)\n",
    "    # Recall we did not apply any activation to our output layer, we need to make our outputs look like probality.\n",
    "    if task == 'c':\n",
    "        w = torch.Tensor([1.0,2.0,8.0])\n",
    "        loss_fn = nn.NLLLoss(weight=w)\n",
    "    if task =='b':\n",
    "#         w = torch.Tensor([9.0,1.0])\n",
    "        loss_fn = nn.BCELoss(weight=None)\n",
    "    if task =='a':\n",
    "        loss_fn = nn.BCELoss()\n",
    "    return model, optimizer, loss_fn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word2idx with train and test set------\n",
      "Begin to get word2vec \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Prepare data for task a---------------\n",
      "---------You are requiring train data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "size of training set:  torch.Size([13240, 105])\n",
      "(15291, 10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    _, tokenized_corpus = get_word2idx()\n",
    "    task = 'a'\n",
    "    output_dim = 3 if task == 'c' else 1\n",
    "    emsize = 10\n",
    "    print('Begin to get word2vec ')\n",
    "    embedding, vocabulary = word2vec_embedding(tokenized_corpus, embed_size=emsize)\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    train_sent_tensor, train_label_tensor = get_model_inputs(train=True, task=task, word2idx=word2idx)\n",
    "    print('size of training set: ', train_sent_tensor.shape)\n",
    "    print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GRU network with embedding on task a\n",
      "GRU(\n",
      "  (embedding): Embedding(15291, 10)\n",
      "  (gru): GRU(10, 8, bidirectional=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch:   1 | Train accuracy: 82.94% | Valid acc: 78.55%\n",
      "Epoch:   1 | Train f1_score: 0.80 | Valid f1_score: 0.74\n",
      "[[7289  658]\n",
      " [1375 2594]]\n",
      "Valid confusion matrix: \n",
      "[[796  97]\n",
      " [187 244]]\n",
      "---------------Prepare data for task a---------------\n",
      "---------You are requiring test data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "Test result for task a generated!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on GRU network with embedding on task\",task)\n",
    "model, optimizer, loss_fn = embed_GRU_model(embedding, Vocabulary=len(word2idx), \n",
    "                                            lr=0.018,EMBEDDING_DIM=emsize, HIDDEN_DIM=8,task=task)\n",
    "print(model)\n",
    "trained_model=trainer(model, optimizer, loss_fn, train_sent_tensor, train_label_tensor,epoch_num=1, \n",
    "        batch_size=32,valid_size=0.1)\n",
    "generate_testing_result(model=trained_model, word2idx=word2idx, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on LSTM network with embedding on task a\n",
      "LSTM(\n",
      "  (embedding): Embedding(15291, 10)\n",
      "  (lstm): LSTM(10, 5, bidirectional=True)\n",
      "  (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "Epoch:   1 | Train accuracy: 80.15% | Valid acc: 76.96%\n",
      "Epoch:   1 | Train f1_score: 0.74 | Valid f1_score: 0.67\n",
      "[[7716  222]\n",
      " [2143 1835]]\n",
      "Valid confusion matrix: \n",
      "[[868  34]\n",
      " [271 151]]\n",
      "---------------Prepare data for task a---------------\n",
      "---------You are requiring test data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "Test result for task a generated!\n"
     ]
    }
   ],
   "source": [
    "# print(train_label_tensor.shape)\n",
    "print(\"Training on LSTM network with embedding on task\",task)\n",
    "model, optimizer, loss_fn = embed_LSTM_model(embedding, Vocabulary=len(word2idx), \n",
    "                                            lr=0.021,EMBEDDING_DIM=emsize, HIDDEN_DIM=5,task=task,OUTPUT_DIM=1)\n",
    "print(model)\n",
    "trained_model=trainer(model, optimizer, loss_fn, train_sent_tensor, train_label_tensor,epoch_num=1, \n",
    "        batch_size=64,valid_size=0.1)\n",
    "generate_testing_result(model=trained_model, word2idx=word2idx, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word2idx with train and test set------\n",
      "Begin to get word2vec \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Prepare data for task b---------------\n",
      "---------You are requiring train data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "size of training set:  torch.Size([4400, 103])\n",
      "(15291, 10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    _, tokenized_corpus = get_word2idx()\n",
    "    task = 'b'\n",
    "    output_dim = 3 if task == 'c' else 1\n",
    "    emsize = 10\n",
    "    print('Begin to get word2vec ')\n",
    "    embedding, vocabulary = word2vec_embedding(tokenized_corpus, embed_size=emsize)\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    train_sent_tensor, train_label_tensor = get_model_inputs(train=True, task=task, word2idx=word2idx)\n",
    "    print('size of training set: ', train_sent_tensor.shape)\n",
    "    print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4235])\n",
      "torch.Size([4235, 103])\n"
     ]
    }
   ],
   "source": [
    "testx = train_sent_tensor[:1000,:]\n",
    "testy = train_label_tensor[:1000]\n",
    "train_sent_tensor=train_sent_tensor[1001:,:]\n",
    "train_label_tensor=train_label_tensor[1001:]\n",
    "balance = train_sent_tensor[train_label_tensor==0,:]\n",
    "d = balance.shape[0]\n",
    "train_sent_tensor=torch.cat((train_sent_tensor,balance), 0) \n",
    "train_sent_tensor=torch.cat((train_sent_tensor,balance), 0) \n",
    "train_label_tensor=torch.cat((train_label_tensor,torch.zeros(2*d)),0)\n",
    "print(train_label_tensor.shape)\n",
    "print(train_sent_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GRU network with embedding on task b\n",
      "GRU(\n",
      "  (embedding): Embedding(15291, 10)\n",
      "  (gru): GRU(10, 5, bidirectional=True)\n",
      "  (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "Epoch:   1 | Train accuracy: 70.73% | Valid acc: 48.84%\n",
      "Epoch:   1 | Train f1_score: 0.43 | Valid f1_score: 0.33\n",
      "[[  27 1206]\n",
      " [  21 2938]]\n",
      "Valid confusion matrix: \n",
      "[[ 0 21]\n",
      " [ 1 21]]\n",
      "Epoch:   2 | Train accuracy: 70.99% | Valid acc: 48.84%\n",
      "Epoch:   2 | Train f1_score: 0.44 | Valid f1_score: 0.33\n",
      "[[  33 1200]\n",
      " [  16 2943]]\n",
      "Valid confusion matrix: \n",
      "[[ 0 21]\n",
      " [ 1 21]]\n",
      "Epoch:   3 | Train accuracy: 76.79% | Valid acc: 53.49%\n",
      "Epoch:   3 | Train f1_score: 0.64 | Valid f1_score: 0.42\n",
      "[[ 355  878]\n",
      " [  95 2864]]\n",
      "Valid confusion matrix: \n",
      "[[ 2 19]\n",
      " [ 1 21]]\n",
      "get acc\n",
      "test acc tensor(0.8400) test f1 0.5555555555555556\n",
      "[[ 20  86]\n",
      " [ 74 820]]\n",
      "---------------Prepare data for task b---------------\n",
      "---------You are requiring test data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "Test result for task b generated!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on GRU network with embedding on task\",task)\n",
    "model, optimizer, loss_fn = embed_GRU_model(embedding, Vocabulary=len(word2idx), lr=0.002,\n",
    "                                            EMBEDDING_DIM=emsize, HIDDEN_DIM=5,task=task)\n",
    "print(model)\n",
    "trained_model=trainer(model, optimizer, loss_fn, train_sent_tensor, train_label_tensor,\n",
    "        epoch_num=3, batch_size=64,valid_size=0.01)\n",
    "pre=trained_model(testx)\n",
    "acc, f1_score, matrix = myaccuracy(pre,testy)\n",
    "print('test acc',acc,'test f1', f1_score)\n",
    "print(matrix)\n",
    "generate_testing_result(model=trained_model, word2idx=word2idx, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on LSTM network with embedding on task b\n",
      "Epoch:   1 | Train accuracy: 70.22% | Valid acc: 70.52%\n",
      "Epoch:   1 | Train f1_score: 0.44 | Valid f1_score: 0.42\n",
      "[[  38 1095]\n",
      " [  40 2638]]\n",
      "Valid confusion matrix: \n",
      "[[  1 120]\n",
      " [  5 298]]\n",
      "Epoch:   2 | Train accuracy: 71.06% | Valid acc: 72.17%\n",
      "Epoch:   2 | Train f1_score: 0.45 | Valid f1_score: 0.45\n",
      "[[  47 1086]\n",
      " [  17 2661]]\n",
      "Valid confusion matrix: \n",
      "[[  4 117]\n",
      " [  1 302]]\n",
      "Epoch:   3 | Train accuracy: 74.15% | Valid acc: 74.76%\n",
      "Epoch:   3 | Train f1_score: 0.57 | Valid f1_score: 0.57\n",
      "[[ 214  919]\n",
      " [  66 2612]]\n",
      "Valid confusion matrix: \n",
      "[[ 23  98]\n",
      " [  9 294]]\n",
      "get acc\n",
      "test acc tensor(0.8540) test f1 0.530697524911604\n",
      "[[ 12  94]\n",
      " [ 52 842]]\n",
      "---------------Prepare data for task b---------------\n",
      "---------You are requiring test data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "Test result for task b generated!\n"
     ]
    }
   ],
   "source": [
    "# print(train_label_tensor.shape)\n",
    "print(\"Training on LSTM network with embedding on task\",task)\n",
    "model, optimizer, loss_fn = embed_LSTM_model(embedding, Vocabulary=len(word2idx), \n",
    "                                            lr=0.002,EMBEDDING_DIM=emsize, HIDDEN_DIM=5,task=task,OUTPUT_DIM=1)\n",
    "trained_model=trainer(model, optimizer, loss_fn, train_sent_tensor, train_label_tensor,\n",
    "        epoch_num=3, batch_size=64,valid_size=0.1)\n",
    "pre=trained_model(testx)\n",
    "acc, f1_score, matrix = myaccuracy(pre,testy)\n",
    "print('test acc',acc,'test f1', f1_score)\n",
    "print(matrix)\n",
    "generate_testing_result(model=trained_model, word2idx=word2idx, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word2idx with train and test set------\n",
      "Begin to get word2vec \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Prepare data for task c---------------\n",
      "---------You are requiring train data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "size of training set:  torch.Size([3876, 103])\n",
      "(15291, 10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    _, tokenized_corpus = get_word2idx()\n",
    "    task = 'c'\n",
    "    output_dim = 3 if task == 'c' else 1\n",
    "    emsize = 10\n",
    "    print('Begin to get word2vec ')\n",
    "    embedding, vocabulary = word2vec_embedding(tokenized_corpus, embed_size=emsize)\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    train_sent_tensor, train_label_tensor = get_model_inputs(train=True, task=task, word2idx=word2idx)\n",
    "    print('size of training set: ', train_sent_tensor.shape)\n",
    "    print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2407])\n",
      "torch.Size([1074])\n",
      "torch.Size([395])\n"
     ]
    }
   ],
   "source": [
    "print(train_label_tensor[train_label_tensor==0].shape)\n",
    "print(train_label_tensor[train_label_tensor==1].shape)\n",
    "print(train_label_tensor[train_label_tensor==2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GRU network with embedding on task c\n",
      "GRU(\n",
      "  (embedding): Embedding(15291, 10)\n",
      "  (gru): GRU(10, 20, bidirectional=True)\n",
      "  (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=80, out_features=3, bias=True)\n",
      ")\n",
      "Epoch:   1 | Train accuracy: 52.32% | Valid acc: 53.09%\n",
      "Train f1score: \n",
      "0.43321123242582704\n",
      "Valid f1score: \n",
      "0.42838827401460494\n",
      "Train confusion matrix: \n",
      "[[1288  336  538]\n",
      " [ 303  429  230]\n",
      " [ 117  139  108]]\n",
      "Valid confusion matrix: \n",
      "[[149  38  58]\n",
      " [ 35  47  30]\n",
      " [  9  12  10]]\n",
      "Epoch:   2 | Train accuracy: 62.47% | Valid acc: 64.18%\n",
      "Train f1score: \n",
      "0.49494980674900885\n",
      "Valid f1score: \n",
      "0.4951412456008044\n",
      "Train confusion matrix: \n",
      "[[1446  519  197]\n",
      " [ 198  671   93]\n",
      " [ 121  181   62]]\n",
      "Valid confusion matrix: \n",
      "[[167  51  27]\n",
      " [ 20  77  15]\n",
      " [ 10  16   5]]\n",
      "Epoch:   3 | Train accuracy: 63.68% | Valid acc: 68.04%\n",
      "Train f1score: \n",
      "0.4916735503642953\n",
      "Valid f1score: \n",
      "0.5238274675913874\n",
      "Train confusion matrix: \n",
      "[[1496  506  160]\n",
      " [ 197  678   87]\n",
      " [ 132  185   47]]\n",
      "Valid confusion matrix: \n",
      "[[176  53  16]\n",
      " [ 18  83  11]\n",
      " [ 10  16   5]]\n",
      "Epoch:   4 | Train accuracy: 65.08% | Valid acc: 68.56%\n",
      "Train f1score: \n",
      "0.49645661872377084\n",
      "Valid f1score: \n",
      "0.5264057579847053\n",
      "Train confusion matrix: \n",
      "[[1559  468  135]\n",
      " [ 210  668   84]\n",
      " [ 138  183   43]]\n",
      "Valid confusion matrix: \n",
      "[[180  51  14]\n",
      " [ 21  81  10]\n",
      " [ 10  16   5]]\n",
      "Epoch:   5 | Train accuracy: 65.60% | Valid acc: 69.07%\n",
      "Train f1score: \n",
      "0.49949188894050595\n",
      "Valid f1score: \n",
      "0.5291062801932368\n",
      "Train confusion matrix: \n",
      "[[1580  455  127]\n",
      " [ 213  665   84]\n",
      " [ 139  182   43]]\n",
      "Valid confusion matrix: \n",
      "[[183  48  14]\n",
      " [ 22  80  10]\n",
      " [ 10  16   5]]\n",
      "Epoch:   6 | Train accuracy: 66.14% | Valid acc: 70.10%\n",
      "Train f1score: \n",
      "0.501672295663202\n",
      "Valid f1score: \n",
      "0.5381263662174726\n",
      "Train confusion matrix: \n",
      "[[1595  450  117]\n",
      " [ 212  671   79]\n",
      " [ 140  183   41]]\n",
      "Valid confusion matrix: \n",
      "[[185  49  11]\n",
      " [ 21  82   9]\n",
      " [ 10  16   5]]\n",
      "Epoch:   7 | Train accuracy: 66.17% | Valid acc: 70.10%\n",
      "Train f1score: \n",
      "0.5008367946125298\n",
      "Valid f1score: \n",
      "0.5369945320795209\n",
      "Train confusion matrix: \n",
      "[[1598  452  112]\n",
      " [ 212  670   80]\n",
      " [ 143  181   40]]\n",
      "Valid confusion matrix: \n",
      "[[186  48  11]\n",
      " [ 21  81  10]\n",
      " [ 10  16   5]]\n",
      "Epoch:   8 | Train accuracy: 66.60% | Valid acc: 69.85%\n",
      "Train f1score: \n",
      "0.5048053573009232\n",
      "Valid f1score: \n",
      "0.5340457703368969\n",
      "Train confusion matrix: \n",
      "[[1609  446  107]\n",
      " [ 211  673   78]\n",
      " [ 143  180   41]]\n",
      "Valid confusion matrix: \n",
      "[[186  47  12]\n",
      " [ 21  80  11]\n",
      " [ 10  16   5]]\n",
      "Epoch:   9 | Train accuracy: 66.49% | Valid acc: 70.10%\n",
      "Train f1score: \n",
      "0.504135029788162\n",
      "Valid f1score: \n",
      "0.5364151523646702\n",
      "Train confusion matrix: \n",
      "[[1604  451  107]\n",
      " [ 211  674   77]\n",
      " [ 142  181   41]]\n",
      "Valid confusion matrix: \n",
      "[[186  47  12]\n",
      " [ 20  81  11]\n",
      " [ 10  16   5]]\n",
      "Epoch:  10 | Train accuracy: 66.54% | Valid acc: 70.10%\n",
      "Train f1score: \n",
      "0.5044821249776908\n",
      "Valid f1score: \n",
      "0.535729387745337\n",
      "Train confusion matrix: \n",
      "[[1607  447  108]\n",
      " [ 212  673   77]\n",
      " [ 144  179   41]]\n",
      "Valid confusion matrix: \n",
      "[[187  46  12]\n",
      " [ 21  80  11]\n",
      " [ 10  16   5]]\n",
      "---------------Prepare data for task c---------------\n",
      "---------You are requiring test data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "Test result for task c generated!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(train_label_tensor.shape)\n",
    "print(\"Training on GRU network with embedding on task\",task)\n",
    "model, optimizer, loss_fn = embed_GRU_model(embedding, Vocabulary=len(word2idx), \n",
    "                                            lr=0.002,EMBEDDING_DIM=emsize, HIDDEN_DIM=20,task=task,OUTPUT_DIM=3)\n",
    "print(model)\n",
    "trained_model=trained_model=trainer(model, optimizer, loss_fn, train_sent_tensor, train_label_tensor,\n",
    "                      epoch_num=10, batch_size=64,valid_size=0.1)\n",
    "generate_testing_result(model=trained_model, word2idx=word2idx, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on LSTM network with embedding on task c\n",
      "LSTM(\n",
      "  (embedding): Embedding(15291, 10)\n",
      "  (lstm): LSTM(10, 20, bidirectional=True)\n",
      "  (bn2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=80, out_features=3, bias=True)\n",
      ")\n",
      "Epoch:   1 | Train accuracy: 51.86% | Valid acc: 53.09%\n",
      "Train f1score: \n",
      "0.4148286202719649\n",
      "Valid f1score: \n",
      "0.41905453569780643\n",
      "Train confusion matrix: \n",
      "[[1164  818  174]\n",
      " [ 271  590  112]\n",
      " [ 118  186   55]]\n",
      "Valid confusion matrix: \n",
      "[[130  97  24]\n",
      " [ 23  71   7]\n",
      " [ 13  18   5]]\n",
      "Epoch:   2 | Train accuracy: 61.87% | Valid acc: 65.98%\n",
      "Train f1score: \n",
      "0.46836744343289527\n",
      "Valid f1score: \n",
      "0.49894710519735197\n",
      "Train confusion matrix: \n",
      "[[1486  534  136]\n",
      " [ 256  634   83]\n",
      " [ 133  188   38]]\n",
      "Valid confusion matrix: \n",
      "[[174  58  19]\n",
      " [ 20  78   3]\n",
      " [ 11  21   4]]\n",
      "Epoch:   3 | Train accuracy: 62.33% | Valid acc: 66.24%\n",
      "Train f1score: \n",
      "0.48136309252865445\n",
      "Valid f1score: \n",
      "0.5285999525392269\n",
      "Train confusion matrix: \n",
      "[[1401  654  101]\n",
      " [ 176  732   65]\n",
      " [ 103  215   41]]\n",
      "Valid confusion matrix: \n",
      "[[164  78   9]\n",
      " [ 12  87   2]\n",
      " [ 11  19   6]]\n",
      "---------------Prepare data for task c---------------\n",
      "---------You are requiring test data!---------\n",
      "------------Begin to get corpus-----------\n",
      "------------Begin to tokenize corpus--------------\n",
      "------------Begin to get vocabulary--------------\n",
      "Test result for task c generated!\n"
     ]
    }
   ],
   "source": [
    "# print(train_label_tensor.shape)\n",
    "print(\"Training on LSTM network with embedding on task\",task)\n",
    "model, optimizer, loss_fn = embed_LSTM_model(embedding, Vocabulary=len(word2idx), \n",
    "                                            lr=0.001,EMBEDDING_DIM=emsize, HIDDEN_DIM=20,task=task,OUTPUT_DIM=3)\n",
    "print(model)\n",
    "trained_model=trainer(model, optimizer, loss_fn, train_sent_tensor, \n",
    "        train_label_tensor,epoch_num=3, batch_size=64,valid_size=0.1)\n",
    "generate_testing_result(model=trained_model, word2idx=word2idx, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
